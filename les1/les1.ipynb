{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We beginnen met een simpele bayes functies die gegeven een aantal conditionele kansen en de algemene kans van een binair label, de kans gaat berekenen op een label gegeven de geziene data. Dit is puur de formule van Bayes invullen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T11:22:26.867661100Z",
     "start_time": "2023-10-07T11:22:26.796604200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9998928686212193\n"
     ]
    }
   ],
   "source": [
    "def bayes(conditional_prob_label, conditional_other_label_prob, prob_of_event):\n",
    "    #TODO: een hoop factoren met elkaar vermenigvuldigen\n",
    "    numerator = conditional_prob_label * prob_of_event\n",
    "    denominator = numerator + (conditional_other_label_prob * (1 - prob_of_event))\n",
    "    return (numerator / denominator)\n",
    "\n",
    "spam_given_money = bayes(0.4, 0.0001, 0.7)\n",
    "print(spam_given_money)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor de classificatie, moet je de noemer zelfs niet uitrekenen. De classifier zoekt de grootste teller en geeft dat terug als antwoord. We kijken nu naar een classificatie met meerdere woorden (realistischer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T11:22:28.021407400Z",
     "start_time": "2023-10-07T11:22:27.989461200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "0.1\n",
      "0.001\n",
      "0.99999\n",
      "0.05\n",
      "0.3\n",
      "Is Spam: HAM\n"
     ]
    }
   ],
   "source": [
    "given_money = [0.4, 0.00001]\n",
    "uganda = [0.1, 0.05]\n",
    "ap_hogeschool = [0.001, 0.3]\n",
    "conditional_data = [given_money, uganda, ap_hogeschool]\n",
    "prob_spam = 0.45\n",
    "data_vector = [0, 1, 1] # uganda en AP Hogeschool, geen 'moneytransfer'\n",
    "\n",
    "def bayes_classifier_spam(conditional_data, prob_spam, data_vector):\n",
    "\n",
    "    #checken voor SPAM:\n",
    "    spam_probability = prob_spam\n",
    "    for i in range(len(conditional_data)):\n",
    "        if data_vector[i] == 1:\n",
    "            print(conditional_data[i][0]) #debugging\n",
    "            spam_probability *= conditional_data[i][0]\n",
    "        else:\n",
    "            print((1 - conditional_data[i][0])) #debugging\n",
    "            spam_probability *= (1 - conditional_data[i][0])\n",
    "    \n",
    "    #checken voor HAM:\n",
    "    not_spam_probability = 1-prob_spam\n",
    "    for i in range(len(conditional_data)):\n",
    "        if data_vector[i] == 1:\n",
    "            print(conditional_data[i][1]) #debugging\n",
    "            not_spam_probability *= conditional_data[i][1]\n",
    "        else:\n",
    "            print((1 - conditional_data[i][1])) #debugging\n",
    "            not_spam_probability *= (1 - conditional_data[i][1])\n",
    "\n",
    "    if (spam_probability > not_spam_probability):\n",
    "        return \"SPAM\"\n",
    "    else:\n",
    "        return \"HAM\"\n",
    "\n",
    "is_spam = bayes_classifier_spam(conditional_data, prob_spam, data_vector)\n",
    "print(\"Is Spam:\", is_spam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oefening\n",
    "We maken nu een volledige classifier aan, die we ook kunnen hertrainen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T11:27:36.252706500Z",
     "start_time": "2023-10-07T11:27:36.221378700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geklassificeerd als: spam\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "class NaiveBayesianClassifier:\n",
    "    def __init__(self):\n",
    "        self.labels = ['spam', 'ham', 'ongekend']\n",
    "        self.label_probabilities = {label: math.log(1/3) for label in self.labels} \n",
    "        self.feature_probabilities = {label: {} for label in self.labels}\n",
    "    \n",
    "    def train(self, label, feature_probabilities):\n",
    "        for feature, probability in feature_probabilities.items():\n",
    "            if feature not in self.feature_probabilities[label]:\n",
    "                self.feature_probabilities[label][feature] = math.log(probability)\n",
    "            else:\n",
    "                self.feature_probabilities[label][feature] += math.log(probability)\n",
    "    \n",
    "    def classify(self, features):\n",
    "        max_probability = float('-inf')  \n",
    "        max_label = None\n",
    "        for label in self.labels:\n",
    "            probability = self.label_probabilities[label]\n",
    "            for feature, feature_value in features.items():\n",
    "                # Ignore unseen features while classifying. \n",
    "                # If this is not desired, an alternative would be to apply Laplace smoothing \n",
    "                if feature in self.feature_probabilities[label]:\n",
    "                    probability += self.feature_probabilities[label][feature] * feature_value\n",
    "            if probability > max_probability:\n",
    "                max_probability = probability\n",
    "                max_label = label\n",
    "        return max_label\n",
    "\n",
    "# Example usage\n",
    "classifier = NaiveBayesianClassifier()\n",
    "\n",
    "# Training data\n",
    "training_data = {\n",
    "    'spam': {'word1': 0.9, 'word2': 0.8},\n",
    "    'ham': {'word1': 0.1, 'word2': 0.2},\n",
    "    'ongekend': {'word1': 0.5, 'word2': 0.4}\n",
    "}\n",
    "\n",
    "for label, features in training_data.items():\n",
    "    classifier.train(label, features) \n",
    "\n",
    "# Classificatie\n",
    "input_features = {'word1': 0, 'word2': 1}\n",
    "result = classifier.classify(input_features)\n",
    "print(\"Geklassificeerd als:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_algorithms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
