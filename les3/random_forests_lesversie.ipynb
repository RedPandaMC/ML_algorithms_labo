{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro: wisdom of the crowd\n",
    "\n",
    "Wisdom of the crowd: veel mensen kunnen samen een redelijke inschatting maken over een bepaalde vraag. Als we verschillende classifiers of regressors verzamelen en een soort *gemiddelde* nemen van hun uitspraken, komen we tot een stabieler, beter antwoord. Deze techniek heet in het algemeen *ensemble learning*, en in het geval van Decision trees spreken we ook wel van een *random forest*. Een bos is tenslotte een verzameling bomen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methoden kunnen bestaan uit verschillende onderling onsamenhangende (losse) classifiers. Het is niet ongewoon, en zelfs de bedoeling, dat de ensemble classifier een betere accuracy heeft dan de beste van haar componenten ! Met andere woorden: uit een aantal minder goede onderdelen kan soms een beter en robuuster onderdeel worden samengesteld. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:23:57.925879Z",
     "start_time": "2023-10-19T15:23:57.861356900Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IMAGES_PATH = Path() / \"images\" / \"ensembles\"\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laat ons starten met een 'voting classifier' die bestaat uit 3 verschillende classifiers, die elk de moons-dataset proberen in te delen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:23:58.337089300Z",
     "start_time": "2023-10-19T15:23:57.861356900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "VotingClassifier(estimators=[('lr', LogisticRegression(random_state=42)),\n                             ('rf', RandomForestClassifier(random_state=42)),\n                             ('svc', SVC(random_state=42))])",
      "text/html": "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression(random_state=42)),\n                             (&#x27;rf&#x27;, RandomForestClassifier(random_state=42)),\n                             (&#x27;svc&#x27;, SVC(random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression(random_state=42)),\n                             (&#x27;rf&#x27;, RandomForestClassifier(random_state=42)),\n                             (&#x27;svc&#x27;, SVC(random_state=42))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>rf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>svc</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(random_state=42)),\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('svc', SVC(random_state=42))\n",
    "    ]\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De individuele onderdelen van deze classifier hebben volgende accuracy scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:23:58.351465300Z",
     "start_time": "2023-10-19T15:23:58.337089300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 0.864\n",
      "rf = 0.896\n",
      "svc = 0.896\n"
     ]
    }
   ],
   "source": [
    "for name, clf in voting_clf.named_estimators_.items():\n",
    "    print(name, \"=\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echter, de ensemble method is in het algemeen nog net iets slimmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:23:58.391021500Z",
     "start_time": "2023-10-19T15:23:58.361485700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.912"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als alle onderdelen van de ensemble methode ook 'class prediction' ondersteunen zoals uitgelegd bij een decision tree, dan kun je nog net iets beter inschatten door de aantallen van al die klasses bij elkaar op te tellen. Voters die zeker zijn van hun stuk (door goede aantallen) krijgen op die manier meer gewicht in het eindresultaat. \n",
    "\n",
    "Om het in onderstaand voorbeeld te laten werken moet je bij een SVC de probability parameter even expliciet op true zetten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:23:58.460385900Z",
     "start_time": "2023-10-19T15:23:58.366010500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.92"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.voting = \"soft\"\n",
    "voting_clf.named_estimators[\"svc\"].probability = True\n",
    "voting_clf.fit(X_train, y_train)\n",
    "voting_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging en Pasting\n",
    "1 manier om verschillende classifiers te hebben is om dezelfde soort te gebruiken, maar op een lichtjes afwijkende dataset. We nemen daarvoor telkens een random deel van de gehele dataset. Er worden dan willekeurige stalen genomen uit een dataset. Naargelang de exacte techniek, gebruiken we volgende terminologie:\n",
    "- bij het willekeurig selecteren kunnen er dubbels optreden: *bagging* (van *bootstrap aggregating*)\n",
    "- bij het willekeurig selecteren laten we geen dubbels toe: *pasting*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:23:58.641341800Z",
     "start_time": "2023-10-19T15:23:58.508891500Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'images/bagging.png'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mIPython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdisplay\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Image\n\u001B[0;32m----> 2\u001B[0m \u001B[43mImage\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimages/bagging.png\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/ml_algorithms_env/lib/python3.11/site-packages/IPython/core/display.py:970\u001B[0m, in \u001B[0;36mImage.__init__\u001B[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata, alt)\u001B[0m\n\u001B[1;32m    968\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munconfined \u001B[38;5;241m=\u001B[39m unconfined\n\u001B[1;32m    969\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malt \u001B[38;5;241m=\u001B[39m alt\n\u001B[0;32m--> 970\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mImage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    971\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    973\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwidth \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwidth\u001B[39m\u001B[38;5;124m'\u001B[39m, {}):\n\u001B[1;32m    974\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwidth \u001B[38;5;241m=\u001B[39m metadata[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwidth\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[0;32m~/miniconda3/envs/ml_algorithms_env/lib/python3.11/site-packages/IPython/core/display.py:327\u001B[0m, in \u001B[0;36mDisplayObject.__init__\u001B[0;34m(self, data, url, filename, metadata)\u001B[0m\n\u001B[1;32m    324\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    325\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m--> 327\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    328\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_data()\n",
      "File \u001B[0;32m~/miniconda3/envs/ml_algorithms_env/lib/python3.11/site-packages/IPython/core/display.py:1005\u001B[0m, in \u001B[0;36mImage.reload\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1003\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Reload the raw data from file or URL.\"\"\"\u001B[39;00m\n\u001B[1;32m   1004\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed:\n\u001B[0;32m-> 1005\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mImage\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1006\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mretina:\n\u001B[1;32m   1007\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retina_shape()\n",
      "File \u001B[0;32m~/miniconda3/envs/ml_algorithms_env/lib/python3.11/site-packages/IPython/core/display.py:353\u001B[0m, in \u001B[0;36mDisplayObject.reload\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    351\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfilename \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    352\u001B[0m     encoding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_read_flags \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 353\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read_flags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m    354\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;241m=\u001B[39m f\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m    355\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39murl \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    356\u001B[0m     \u001B[38;5;66;03m# Deferred import\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'images/bagging.png'"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='images/bagging.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wanneer alle subcomponenten zijn getrained, kan er een gehele uitspraak worden gedaan:\n",
    "- voor classificatie wordt er meestal gekozen voor een 'statistische keuze', typisch de meest voorkomende voorspelling (majority vote)\n",
    "- voor regressie wordt er meestal gekozen voor het gemiddelde.\n",
    "\n",
    "Door de [wet van de grote getallen](https://nl.wikipedia.org/wiki/Wetten_van_de_grote_aantallen), gaat zowel de bias als de variantie naar beneden. Herinner:\n",
    "- Bias: foute veronderstellingen, zoals een dataset met duidelijke 'kromming' lineair proberen te fitten. Een model met hoge bias is waarschijnlijk een underfit\n",
    "- Variance: sensitiviteit voor variatie in de data; een model dat veel vrijheidsgraden heeft en de trainingsdata (te) goed fit, is mogelijk een overfit model.\n",
    "Naast die twee oorzaken van fouten, die we natuurlijk als modelmakers zo klein mogelijk proberen te krijgen, is er nog een derde niet te vergeten aspect:\n",
    "- Ruis: in het Engels noise genoemd: deze fout, bvb door de beperking van een meettoestel, door de aard van het probleem (sommige dingen zijn nu éénmaal onderhevig aan kans en dus wat 'ruis'). Hier hebben we, buiten misschien het goed cleansen van de data, geen vat op.\n",
    "\n",
    "Omdat de subcomponenten onafhankelijk van mekaar kunnen rekenen, kan dit process parallel worden uitgevoerd en is deze methode dus heel goed schaalbaar.\n",
    "\n",
    "Implmentatie gebeurt als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-19T15:23:58.641341800Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
    "                            max_samples=100, n_jobs=-1, random_state=42)\n",
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Er worden 500 classifiers getrained, en elke Decision Tree classifier gebruikt 100 datapunten. De parameter n_jobs = -1 geeft aan dat we alle cores van de beschikbare machine willen gebruiken. je kan dit bijvoorbeeld hier beperken tot een vast aantal indien gewenst.\n",
    "\n",
    "Opgelet: voor max_samples is de parameter overloaded: als je een integer waarde geeft, dan geef je expliciet aan hoeveel items er moeten worden geselecteerd. Als je een float tussen 0 en 1 geeft, dan duidt je aan welk proportioneel deel van de training date je wil gebruiken, bijvoorbeeld 0.5 voor de helft per estimator.\n",
    "\n",
    "We vergelijken nu een enkele decision tree met onze 500 decision trees met bagging:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:23:58.641341800Z",
     "start_time": "2023-10-19T15:23:58.641341800Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_decision_boundary(clf, X, y, alpha=1.0):\n",
    "    axes=[-1.5, 2.4, -1, 1.5]\n",
    "    x1, x2 = np.meshgrid(np.linspace(axes[0], axes[1], 100),\n",
    "                         np.linspace(axes[2], axes[3], 100))\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    \n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3 * alpha, cmap='Wistia')\n",
    "    plt.contour(x1, x2, y_pred, cmap=\"Greys\", alpha=0.8 * alpha)\n",
    "    colors = [\"#78785c\", \"#c47b27\"]\n",
    "    markers = (\"o\", \"^\")\n",
    "    for idx in (0, 1):\n",
    "        plt.plot(X[:, 0][y == idx], X[:, 1][y == idx],\n",
    "                 color=colors[idx], marker=markers[idx], linestyle=\"none\")\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(r\"$x_1$\")\n",
    "    plt.ylabel(r\"$x_2$\", rotation=0)\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_decision_boundary(tree_clf, X_train, y_train)\n",
    "plt.title(\"Decision Tree\")\n",
    "plt.sca(axes[1])\n",
    "plot_decision_boundary(bag_clf, X_train, y_train)\n",
    "plt.title(\"Decision Trees with Bagging\")\n",
    "plt.ylabel(\"\")\n",
    "#save_fig(\"decision_tree_without_and_with_bagging_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-bag (oob) evaluatie\n",
    "\n",
    "Het standaard gedrag van bagging selecteert per boom evenveel items als de training set, maar kiest deze met teruglegging. Hierdoor zullen bijna zeker niet alle items wordt geselecteerd, maar typisch slechts 63%. De items die niet worden gekozen, noemen we de *out-of-bag*-instances (oob). Daardoor kan een ensemble worden geëvalueerd zonder de nood aan een extra valuation set: als er veel estimators (onderdelen van het ensemble model) zijn, dan is elk item uit de training set een oob-item voor een aantal estimators. Op deze manier kun je als de dit doet voor de hele training set, de accuracy van het gehele ensemble model berekenen, dit noemen we de *out-of-bag-score* of oob-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:23:58.641341800Z",
     "start_time": "2023-10-19T15:23:58.641341800Z"
    }
   },
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
    "                            oob_score=True, n_jobs=-1, random_state=42)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "je kunt ook voor een item de oob-decision functie aanroepen, eg de voorspelde klasse op basis van de predictors die deze data niet bevatten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:23:58.641341800Z",
     "start_time": "2023-10-19T15:23:58.641341800Z"
    }
   },
   "outputs": [],
   "source": [
    "bag_clf.oob_decision_function_[:3]  # probas for the first 3 instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:23:58.641341800Z",
     "start_time": "2023-10-19T15:23:58.641341800Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random patches\n",
    "\n",
    "Het is ook mogelijk om, naast de variatie in training samples per estimator, ook te variëren in welke features je meegeeft aan elke estimator. Dit noemen we *Random patches*. Wanneer we enkel de features variëren, maar de samples wel allemaal gebruiken bij elke estimator, spreken we van *random subspaces*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naast de *BaggingClassifier* is er ook een specifieke *RandomForestClassifier*, die is net iets handiger om mee te werken als je met onderliggende Decision Trees werkt. Het random forest algoritme werkt een beetje anders: het laat namelijk elke decision tree voor elke stap niet de beste feature zoeken om te splitsen; maar beperkt zich tot een random selectie van features. Op die manier is er opnieuw meer variatie, wat weliswaar leidt tot meer bias, maar minder risico op overfitting, wat voor decision trees sowieso het grootste risico is. Dit kan je op zich ook afdwingen in een BaggingClassifier door de parameter max_features = 'sqrt' mee te geven, maar zoals gezegd is het ingebakken voor een RandomForrestClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-19T15:23:58.641341800Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16,\n",
    "                                 n_jobs=-1, random_state=42)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:23:58.649768600Z",
     "start_time": "2023-10-19T15:23:58.649768600Z"
    }
   },
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(max_features=\"sqrt\", max_leaf_nodes=16),\n",
    "    n_estimators=500, n_jobs=-1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ze zijn effectief identiek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-19T15:23:58.649768600Z"
    }
   },
   "outputs": [],
   "source": [
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred_bag = bag_clf.predict(X_test)\n",
    "np.all(y_pred_bag == y_pred_rf)  # zelfde predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "Om eventueel te bekijken in de labos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het idee achter boosting is om in plaats van de onderliggende predictors van een ensemble method onafhankelijk van elkaar te trainen, deze sequentieel te trainen: het ene model neemt de output van de ene als input. Op die manier 'verbeteren' de methodes elkaar. Er zijn heel veel verschillende technieken, waarvan we er een tweetal zullen bekijken:\n",
    "- AdaBoost\n",
    "- Gradient Boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:23:58.652277900Z",
     "start_time": "2023-10-19T15:23:58.652277900Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "#Image(filename='images/boosting.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "\n",
    "Adaboost werkt door training items die specifiek slecht werden getrained door een classifier, meer gewicht te geven voor de volgende classifier, om deze dus te forceren hier meer aandacht aan te besteden. Er wordt dus na elke classifier al een evaluatie gemaakt en deze evaluatie is van belang voor de trainingsfase van de volgende classifier, etc. Het volgend voorbeeld toont de decision boundaries van 5 opeenvolgende predictors in de moons dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:23:58.652277900Z",
     "start_time": "2023-10-19T15:23:58.652277900Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "m = len(X_train)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "for subplot, learning_rate in ((0, 1), (1, 0.5)):\n",
    "    sample_weights = np.ones(m) / m\n",
    "    plt.sca(axes[subplot])\n",
    "    for i in range(5):\n",
    "        svm_clf = SVC(C=0.2, gamma=0.6, random_state=42)\n",
    "        svm_clf.fit(X_train, y_train, sample_weight=sample_weights * m)\n",
    "        y_pred = svm_clf.predict(X_train)\n",
    "\n",
    "        error_weights = sample_weights[y_pred != y_train].sum()\n",
    "        r = error_weights / sample_weights.sum()  # equation 7-1\n",
    "        alpha = learning_rate * np.log((1 - r) / r)  # equation 7-2\n",
    "        sample_weights[y_pred != y_train] *= np.exp(alpha)  # equation 7-3\n",
    "        sample_weights /= sample_weights.sum()  # normalization step\n",
    "\n",
    "        plot_decision_boundary(svm_clf, X_train, y_train, alpha=0.4)\n",
    "        plt.title(f\"learning_rate = {learning_rate}\")\n",
    "    if subplot == 0:\n",
    "        plt.text(-0.75, -0.95, \"1\", fontsize=16)\n",
    "        plt.text(-1.05, -0.95, \"2\", fontsize=16)\n",
    "        plt.text(1.0, -0.95, \"3\", fontsize=16)\n",
    "        plt.text(-1.45, -0.5, \"4\", fontsize=16)\n",
    "        plt.text(1.36,  -0.95, \"5\", fontsize=16)\n",
    "    else:\n",
    "        plt.ylabel(\"\")\n",
    "\n",
    "save_fig(\"boosting_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "De exacte manier van hoe deze gewichten na evaluatie worden bepaald, zou ons hier te ver leiden, maar dit is hoe je adaBoost kan gebruiken in de praktijk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-19T15:23:58.652277900Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=30,\n",
    "    learning_rate=0.5, random_state=42)\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-19T15:23:58.652277900Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary(ada_clf, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "Deze method gaat elke volgende predictor gebruiken om de *fouten* van de vorige predictor te voorspellen, en werkt vooral goed voor regressie. Op die manier is de totale voorspelling dan de som van de onderliggende predictors in de ensemble methode. We bekijken als voorbeeld volgende kwadratische dataset met nogal wat ruis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:23:58.652277900Z",
     "start_time": "2023-10-19T15:23:58.652277900Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3 * X[:, 0] ** 2 + 0.05 * np.random.randn(100)  # y = 3x² + Gaussian noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierop fitten we eerst een simpel model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-19T15:23:58.652277900Z"
    }
   },
   "outputs": [],
   "source": [
    "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-19T15:23:58.652277900Z"
    }
   },
   "outputs": [],
   "source": [
    "#plot maken\n",
    "def plot_predictions(regressors, X, y, axes, style,\n",
    "                     label=None, data_style=\"b.\", data_label=None):\n",
    "    x1 = np.linspace(axes[0], axes[1], 500)\n",
    "    y_pred = sum(regressor.predict(x1.reshape(-1, 1))\n",
    "                 for regressor in regressors)\n",
    "    plt.plot(X[:, 0], y, data_style, label=data_label)\n",
    "    plt.plot(x1, y_pred, style, linewidth=2, label=label)\n",
    "    if label or data_label:\n",
    "        plt.legend(loc=\"upper center\")\n",
    "    plt.axis(axes)\n",
    "\n",
    "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.2, 0.8], style=\"g-\",\n",
    "                 label=\"$h_1(x_1)$\", data_label=\"Training set\")\n",
    "plt.title(\"1 Decision Tree Regressor\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu kunnen we een regressie loslaten op de fout die deze regressor elke keer maakt. Vervolgens herhalen we het process een paar keer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:23:58.658787600Z",
     "start_time": "2023-10-19T15:23:58.658787600Z"
    }
   },
   "outputs": [],
   "source": [
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=43)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-19T15:23:58.658787600Z"
    }
   },
   "outputs": [],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=44)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-19T15:23:58.658787600Z"
    }
   },
   "outputs": [],
   "source": [
    "#een finale voorspelling maken: alles optellen\n",
    "X_new = np.array([[-0.4], [0.], [0.5]])\n",
    "sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-19T15:23:58.658787600Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11, 11))\n",
    "\n",
    "plt.subplot(3, 2, 1)\n",
    "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.2, 0.8], style=\"g-\",\n",
    "                 label=\"$h_1(x_1)$\", data_label=\"Training set\")\n",
    "plt.ylabel(\"$y$  \", rotation=0)\n",
    "plt.title(\"Residuals and tree predictions\")\n",
    "\n",
    "plt.subplot(3, 2, 2)\n",
    "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.2, 0.8], style=\"r-\",\n",
    "                 label=\"$h(x_1) = h_1(x_1)$\", data_label=\"Training set\")\n",
    "plt.title(\"Ensemble predictions\")\n",
    "\n",
    "plt.subplot(3, 2, 3)\n",
    "plot_predictions([tree_reg2], X, y2, axes=[-0.5, 0.5, -0.4, 0.6], style=\"g-\",\n",
    "                 label=\"$h_2(x_1)$\", data_style=\"k+\",\n",
    "                 data_label=\"Residuals: $y - h_1(x_1)$\")\n",
    "plt.ylabel(\"$y$  \", rotation=0)\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "plot_predictions([tree_reg1, tree_reg2], X, y, axes=[-0.5, 0.5, -0.2, 0.8],\n",
    "                  style=\"r-\", label=\"$h(x_1) = h_1(x_1) + h_2(x_1)$\")\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "plot_predictions([tree_reg3], X, y3, axes=[-0.5, 0.5, -0.4, 0.6], style=\"g-\",\n",
    "                 label=\"$h_3(x_1)$\", data_style=\"k+\",\n",
    "                 data_label=\"Residuals: $y - h_1(x_1) - h_2(x_1)$\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$  \", rotation=0)\n",
    "\n",
    "plt.subplot(3, 2, 6)\n",
    "plot_predictions([tree_reg1, tree_reg2, tree_reg3], X, y,\n",
    "                 axes=[-0.5, 0.5, -0.2, 0.8], style=\"r-\",\n",
    "                 label=\"$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1)$\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "\n",
    "#save_fig(\"gradient_boosting_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dit kan een stuk simpeler door 1 ingebakken methode te gebruiken. De learning rate bepaalt hoe hard elke boom meetelt: bij een lagere waarde gaat het process trager, maar is er minder kans op overfitting. Deze techniek, meer algemeen het kiezen van waardes waardoor er meer iteraties nodig zijn maar de algemene oplossing beter is, heet *shrinkage*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-19T15:23:58.658787600Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3,\n",
    "                                 learning_rate=1.0, random_state=42)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "XGBoost is een populaire boost libary. Ze is heel efficiënt en wordt in de praktijk vaak gebruikt als alternatief voor Gradient Boosting. Deze library moet je apart installeren ([documentatie](https://xgboost.readthedocs.io/en/stable/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:23:58.668307700Z",
     "start_time": "2023-10-19T15:23:58.658787600Z"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor()\n",
    "xgb_reg.fit(X,y)\n",
    "\n",
    "y_pred = xgb_reg.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping is ingebouwd bij XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-19T15:23:58.658787600Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb_reg.fit(X_train,y_train, eval_set=[(X, y)], early_stopping_rounds=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wanneer boosting gebruiken ?\n",
    "\n",
    "Boosting is aangeraden wanneer de onderdelen van het ensemble model, ook wel de *weak learners* genoemd, op zichzelf geen fantastisch resultaat leveren. Iteratieve modellen kan je gebruiken wanneer:\n",
    "- bagging en pasting niet goed werken\n",
    "- een complex probleem met hoge predictiekwaliteit is nodig\n",
    "- bias en variantie moeten laag blijven, voor een goede veralgemening op nieuwe ongeziene data.\n",
    "\n",
    "\n",
    "#### En welke boosting dan ?\n",
    "- AdaBoost\n",
    "    - vooral voor datasets met veel ruis en outliers\n",
    "    - werkt goed met heel simpele weak learners\n",
    "- Gradient Boosting:\n",
    "    - Dit is de default keuze\n",
    "    - Er zijn meer parameters, dus je kan meer finetunen dan AdaBoost. Maar daar heb je dan ook meer werk mee\n",
    "- XGBoost\n",
    "    - Te overwegen wanneer Gradient Boost te lang duurt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tot nu toe was het samennemen van de verschillende onderdelen bij boosting en stacking een simpele functie (een som, het meeste aantal voorkomende antwoorden, ...). We zouden echter dit ook op een slimmere manier kunnen doen, door aan een laatste model te vragen om op basis van alle onderdelen een slimme combinatie te maken, die het model zelf al trainend kan leren. Dit noemen we tot slot *stacking*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-19T15:23:58.668307700Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "#Image(filename='images/stacking.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dit meta-model, wat alle vorige andere samenvoegt, noemen we ook wel een *blender*. Dit gaan we bekijken in de oefeningen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_algorithms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
